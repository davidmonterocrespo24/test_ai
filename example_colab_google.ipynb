{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+wmyZsf6ZAPqBshhcMFx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidmonterocrespo24/test_ai/blob/main/example_colab_google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entrenar un modelo de lenguaje GPT-2 en Python usando el código de una carpeta, necesitarás seguir los siguientes pasos:\n",
        "\n",
        "Instalar las dependencias necesarias:"
      ],
      "metadata": {
        "id": "ImSdKZbPObA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QWpLsteOV5H",
        "outputId": "2be7352c-1bee-4eed-fa5d-e91c7e79a0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.3.0\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_datasets==3.2.1\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (3.20.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (1.14.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (2.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (0.18.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (23.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (1.22.4)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (2.27.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow_datasets==3.2.1) (2.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.2.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.2.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.2.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.2.1) (2022.12.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tensorflow_datasets==3.2.1) (1.59.0)\n",
            "Installing collected packages: dill, tensorflow_datasets\n",
            "  Attempting uninstall: tensorflow_datasets\n",
            "    Found existing installation: tensorflow-datasets 4.8.3\n",
            "    Uninstalling tensorflow-datasets-4.8.3:\n",
            "      Successfully uninstalled tensorflow-datasets-4.8.3\n",
            "Successfully installed dill-0.3.6 tensorflow_datasets-3.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.5.1\n",
            "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==3.5.1) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from transformers==3.5.1) (23.1)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91.tar.gz (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "!pip install tensorflow_datasets==3.2.1\n",
        "!pip install transformers==3.5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar las bibliotecas necesarias:\n"
      ],
      "metadata": {
        "id": "WYPzgk1ZOfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import os\n"
      ],
      "metadata": {
        "id": "praBxQrgOhtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos:"
      ],
      "metadata": {
        "id": "AIgFPUlHOjzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'sample_data'\n",
        "text = ''\n",
        "for filename in os.listdir(path):\n",
        "    with open(os.path.join(path, filename), 'r') as f:\n",
        "        text += f.read()\n"
      ],
      "metadata": {
        "id": "pU5ZoEGXOlO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizar los datos:"
      ],
      "metadata": {
        "id": "axuE8EG5OpeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo y el tokenizador\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Ajustar la longitud máxima de secuencia\n",
        "model.config.max_position_embeddings = 102048\n",
        "tokenizer.model_max_length = 102048\n",
        "tokenized_text = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "QGlwg5z5Onqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear los conjuntos de entrenamiento y prueba:"
      ],
      "metadata": {
        "id": "jaAknmeiOtAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(tokenized_text)-block_size+1, block_size):\n",
        "    examples.append(tokenized_text[i:i+block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "    inputs.append(ex[:-1])\n",
        "    labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "Nhe91aqiOvae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir el modelo:\n"
      ],
      "metadata": {
        "id": "jQN1X1kPOxHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "id": "HBPqXr1gOy9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilar el modelo:"
      ],
      "metadata": {
        "id": "yCNBUxI1O2Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), loss=[loss_fn, *[None] * model.config.n_layer])\n"
      ],
      "metadata": {
        "id": "sp0Pe2FFO0h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo:"
      ],
      "metadata": {
        "id": "nNNpBozBO8mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "model.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "bnQOz7HeO9u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo:"
      ],
      "metadata": {
        "id": "wFWE5I9hPDwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('modelo')"
      ],
      "metadata": {
        "id": "DtUSy_oiPANS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí te muestro un ejemplo de cómo utilizar el modelo entrenado anteriormente para generar texto:"
      ],
      "metadata": {
        "id": "HvtAkRi-PIXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model = TFGPT2LMHeadModel.from_pretrained('modelo')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Generar texto\n",
        "prompt = \"En un lugar de la Mancha\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "output = model.generate(input_ids, max_length=100, do_sample=True, top_k=50)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "2I1Fk5R1PLEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}